<!DOCTYPE html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>

@import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz:400,700);
@import url(https://fonts.googleapis.com/css?family=Contrail+One:400,700);

</style>
<link rel="stylesheet" type="text/css" href="../../library/common.css" />
<link rel="stylesheet" type="text/css" href="../../library/screen.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../../library/print.css" media="print" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>

</head>
<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Introduction to ML  </span></h1></br> <h2><p class="grey">Master, UGE</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>



<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Supervised learning</span></h1></br> <h2><p class="grey">Master, UGE</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>

<section style="background-color:white;color:black">

<h1 class="green" >Classification ?</h1>


<ul>
<li> Supervised learning
<li> Data : 
</ul>
Trainning data :
$$\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_N,y_N)\}, \mathbf{x_i} \in \mathcal{R}^p,\,y_i \in \{G_1,...,G_K\}$$

<h3 class="red">Goal : </h3>

Find a function $f$ : such that $f(x) \sim y$ to predict $y$ for new $x$</br>
<span class="green">Example : </span> predict if an e-mail is a spam or not from content and meta-data
</section>

<section style="background-color:white;color:black">

<h1 class="green" >Classification ?</h1>
<ul>
<li> $\neq$ Regression where $y$ is a real value
</ul>
Data :
$$\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_N,y_N)\}, \mathbf{x_i} \in \mathcal{R}^p,\,y_i \in \mathbb{R}$$

<h3 class="red">Objectif : </h3>

Find $f$ : $f(x) \sim y$ to predict $y\in\mathcal{R}$ for new values of $x$</br>
<span class="green">Example : </span> y=electrical consumption, x=houseold information, house description, meteo,calendar information
</section>

<section style="background-color:white;color:black">
<h1>Regression</h1>
<img src="./images/kernel_ridge.png" height="65%">
</section>

<section style="background-color:white;color:black">
<h1>Classification</h1>
<img src="./images/ex_classif.png" height="65%">
</section>

<section style="background-color:white;color:black">
<h1>Classification, for prediction</h1>
<img src="./images/ex_classif_test.png" height="65%">
</section>

<section style="background-color:white;color:black">
<h1 class="green" >Supervised learning</h1>
Hypothesys space:
<ul>
<li> $f$ belong to a familly of function $\mathcal{H}$ defined by the methods and the hyper-parameters (SVM, MLP, Decision Tree, Random Forest,...):
$f\in\mathcal{H}$
<li> find the best $f$ in $\mathcal{H}$
</ul>
<h2 class="red"> Goal minimize the generalisation error</h2>
$$R(f)=\mathbb{E}_{\mathcal{X},\mathcal{y}}[L(f(\mathbf{x}),y)]=\int_{\mathcal{X}}\int_{\mathcal{y}}L(f(\mathbf{x}),y)p(\mathbf{x},y)d\mathbf{x}dy$$
$L(.,.)$ : loss function that quantify the errors of $f$ 
</section>


<section style="background-color:white;color:black">
<h2 class="red">Regression, loss function :</h2>
$\mathcal{Y}=\mathbb{R}$, mean square error :
$L(f(\mathbf{x},y)=(f(\mathbf{x})-y)^2$
<img src="./images/kernel_ridge.png" height="55%" style="margin-top:1em">
<span class="green">Example : </span> y=electrical consumption, x=houseold information, house description, meteo,calendar information
</section>

<section style="background-color:white;color:black">

<h2 class="red">Classification, loss function :</h2>
$\mathcal{Y}=\{0,1\}$ (discret set of outcomes), classification error :
$L(f(\mathbf{x}),y)=\mathbf{1}_{f(\mathbf{x})\neq y}$
<img src="./images/ex_classif.png" height="55%" style="margin-top:1em">
</br>
Ex: $y:\{spam, notspam\}$, $\mathbf{x}$ : e-mail data
</section>

<section style="background-color:white;color:black">
<h2 class="red">Empirical risk</h2>
$S=(\mathbf{x},y):\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_n,y_n)\}$
$$R_S(f)=\frac{1}{n}\sum_{i=1}^nL(f(\mathbf{x}_i),y_i)$$
$$\mathbb{E}[R_S(f)]=R(f)$$
</section>

<section style="background-color:white;color:black">
<h2 class="red">Empirical risk minimisation</h2>
$f$ is described by a set of parameters $\color{red}{\mathbf{w}}$, 
$$f(\mathbf{x};\color{red}{\mathbf{w}})$$
Select $f$ (and thus $\color{red}{\mathbf{w}}$) who minimize the empirical risk 
$$\color{red}{\mathbf{\hat{w}}}=\arg\min_{\color{red}{\mathbf{w}}}R_{S_a}(\color{red}{\mathbf{w}})=\arg\min_{\color{red}{\mathbf{w}}}\frac{1}{n}\sum_{i=1}^nL(f(\mathbf{x}_i,\color{red}{\mathbf{w}}),y_i)$$
</section>


<section style="background-color:white;color:black">
<h1 class="green" ></h1>
<h3> Training set / Test set</h3>
$f(\mathbf{x})=f(\mathbf{x},\color{red}{\mathbf{w}})$ may be more or less complex (control the complexity)
<img src="./images/biais_variance.png" height=50%>
<span class="red"> ! don't memorize the training data must work on new sample
</section>


<section style="background-color:white;color:black">
<h1 class="green" >Classification supervisé</h1>
<h3> Trainning data / Test data</h3>
<ul>
<li> unbiased estimator of generalisation error $\sim 70\%$ (Training), $\sim 30\%$ (Test)
<li> Try to estimate the generalisation error
</ul>
<h3 class="red">Cross validation: </h3>
<ul> 
<li> split in $V$ groups.
<li> for each group estimate the empirical risk on the group with remaining as training data 
<li> average the error
</ul>
</section>



<section style="background-color:white;color:black">

<h1 class="green" >Classification, </h1>
how to measure the performance of $f$
<h4>Confusion matrix</h4>
$$C_{kl}\, = \#(y_i=c_k \cap \hat{f}(\mathbf{x}_i)=c_l)$$
<img src="./images/confusion.png" height="50%">
</section>


<section style="background-color:white;color:black">

<h1 class="green" >Classification,</h1>
 how to measure the performance of $f$
<h4>Confusion matrix</h4>
$$C_{kl}\, = \#(y_i=c_k \cap \hat{f}(\mathbf{x}_i)=c_l)$$
<img src="./images/contable.png" height="50%">
https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers
</section>




<section style="padding-top:5em">
<h1>Some classifiers</h1>
</section>


<section style="padding-top:5em;text-align:center">
<h1>K nearest neighbours</h1>
</section>

<section style="background-color:white;color:black">
<h1 class="blue">K nearest neighbours</h1>
To predict the class of a new point $\mathbf{x}$:
<ul>
<li> Find the k points $\{i_1,...,i_k\}$ in the training data that are the nearest neighbours of $\mathbf{x}$
<li> Make them vote by majority to predict the label
</ul>
<h3>Remarks</h3>
<ul>
<li>Hyper-Parameter $k$, 
<li>No true learning algorithm = just memorize the training data, 
<li>Computational time for prediction != learning
</ul>
</section>

<section style="padding-top:5em;text-align:center">
<h1>Regression logistic</h1>
</section>

<section style="background-color:white;color:black">
<h1 class="blue">Regression logistic</h1>
$$P(y=c_k|\mathbf{x})=\frac{exp(\beta_k^{t}\mathbf{x})}{\sum_{h=1}^{K}exp(\beta_h^{t}\mathbf{x})}$$
<ul>
<li>Learning = find the $\beta$ by maximum likelihood
$$\hat{\beta_1},...,\hat{\beta}_k=\arg\max \prod_{i=1}^{N}\prod_{k=1}^{K}P(y=c_k|\mathbf{x_i})^{1_{y_i=G_k}}$$
<li>loss function = categorical cross entropy = $−(y\log(p)+(1−y)\log(1−p))$
<li>convex optimisation problem
<li>Decision boundaries are linear
<li>
</ul>
</section>


<section style="padding-top:5em;text-align:center">
<h1>Support</h1>
<h1>Vector</h1>
<h1>Machine</h1>
</section>

<section style="padding-top:5em;text-align:center">
<h1>S</h1>
<h1>V</h1>
<h1>M</h1>
</section>
<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Optimal decision boundary</h3>
separable case
<img src="images/svm1.png" height="60%">
</section>


<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Optimal decision boundary</h3>
separable case
<img src="images/svm2.png" height="60%">
</section>

<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Optimal decision boundary</h3>
separable case
<img src="images/svm3.png" height="60%">
</section>

<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Optimal decision boundary</h3>
separable case
<img src="images/svm4.png" height="60%">
</section>

<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Optimal decision boundary</h3>
separable case
<img src="images/svm4.png" height="30%">
with $y_i\in\{-1,1\}$ = binary case
$$\hat{w},\hat{b} = \arg\min_{w}||w||$$
with $y_i(w^t\mathbf{w}+b)-1>0,\,\forall i \in\{1,...,N\}$
</section>


<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Optimal decision boundary</h3>
non separable case
<img src="images/svm5.png" height="60%">
</section>



<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Optimal decision boundary</h3>
non separable case
<img src="images/svm5.png" height="30%">
$$\hat{w},\hat{b} = \arg\min_{w}||w||+C\sum_{i=1}^{N}max(0,1-y_i(w^t\mathbf{w}+b))$$
</section>

<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Remarks </h3>
<ul>
<li> convex problem
<li> can be "Kernelised" = re-written by using only dot products
<li> parcimonious solution that only use the 
<h3 class="purple">"supports vectors"</h3>
</ul>
</section>





<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Kernel</h3>
<ul>
<li> Kernel = do ptoduct but in a different space
<li> non linear trabsformation
<li> classical kernel = gaussian kernel </br>
(dot product in an infinite space !)
</ul>
<img src="./images/ex_kernel.png" height="50%">
</section>

<section style="padding-top:5em;text-align:center">


<h1>Decision</h1>
<h1>Tree</h1>
</section>


<section style="background-color:white;color:black">
<h1 class="blue">Decision Tree</h1>
<img src="./images/Arbre_de_decision.jpg" height="50%">
</section>

<section style="background-color:white;color:black">
<h1 class="blue">Decision Tree</h1>
<ul>
<li> iterative greedy search of optimal one dimensional split </br>
<p style="text-align:center">= "information gain -> decrease in entropy"</p>
$$H(p)=-p\log(p)-(1-p)\log(1-p)$$
$$G = H_{before}-(p_{right}.H_{right}+p_{left}.H_{left})$$
<li> until all leafs are "pure" (H=0)
</ul>
Remarks:
<ul>
<li> split = rectangular space partionning
<li> prediction majoirty vote in the sample leaf 
<li> ! over-fitting -> prunning
</ul>
</section>

<section style="background-color:white;color:black">

<h1>Random forest</h1>
<img src="./images/randomForest1.png" height="70%">
</section>

<section style="background-color:white;color:black">

<h1>Random forest</h1>
<ul>
<li> build a set of tree (small control #splits)</br><span class="red"> ! as independant as possible (data perturbation)</span></br>
and combine
Example : <ul>
<li>sample the training data 
<li>sample the features
</ul>
<li> Combination  <span class="red">majority vote</span>
<li> Avoid over-fitting
</ul>
</section>



<section style="background-color:white;color:black">
<h1 class="blue"></h1>
<h3> with scikit-lean </h3>

clf = RandomForestClassifier(n_estimators=20)</br>
scores = cross_val_score(clf, X_train, y_train, scoring='f1_weighted')</br>
scores.mean()  </br>
</section>




<section>
	<h1>additional materials</h1>
Books

<ul>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/"> Element of statistical Learning </a>
	<li> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf"> Pattern recognition and Machine Learning</a>
</ul>


</section>

<script src="../../library/d3.v3.min.js"></script>
<script src="../../library/stack.v1.min.js"></script>
<link rel="stylesheet" href="../../library/styles/hybrid.css">
<script src="../../library/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script>

var mystack = stack()
    .on("activate", activate)
    .on("deactivate", deactivate);

var section = d3.selectAll("section"),
    follow = d3.select("#follow"),
    followAnchor = d3.select("#follow-anchor"),
    lorenz = d3.select("#lorenz"),
    followIndex = section[0].indexOf(follow.node()),
    lorenzIndex = section[0].indexOf(lorenz.node());

function refollow() {
  followAnchor.style("top", (followIndex + (1 - mystack.scrollRatio()) / 2 - d3.event.offset) * 100 + "%");
}

function activate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", refollow);
  if (i === lorenzIndex) startLorenz();
}

function deactivate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", null);
  if (i === lorenzIndex) stopLorenz();
}


</script>
