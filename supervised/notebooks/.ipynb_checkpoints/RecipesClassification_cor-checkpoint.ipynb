{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe classification\n",
    "\n",
    "Imagine a colaborative recipe website chich want to propose an automatic tag function which categorize a proposed recipe among a set of recipe type like french, mexican, italian, korean, .... This functionality will be build thanks to a classifier wich will take as input the ingredients of the recipe and will output the recipe type.\n",
    "\n",
    "In this exercice we will build such a classifier using a set of labelled recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "First we need to read and encode the dataset for further processing. The \"recipe_train.json\" contains an array of recipe with a cuisine filed with the recipe cuisine type (a string) and an ingredients fileds with an array of ingredients (strings). We first load the data and build an np.array of cuisine type to predict (y) and a list of recipes ingredients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./recipes_train.json\") as f:\n",
    "    data_train=json.load(f)\n",
    "    y = np.array([recipe[\"cuisine\"] for recipe in data_train])\n",
    "    xtext = [recipe[\"ingredients\"] for recipe in data_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must encode the ingredients and define the numeric features that will describe our recipes. To do so we will take a classic approach in text processing called bag of words so in our case bag of ingredients. Each recipe will be then associated with a big vectors of zeros and one. Each element of this vector will correspond to an ingredients and we will put a one if the recipe use this ingredients and a zero if it's not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so we start by computing the list of all the ingredients and a dict which associated to each ingredients an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = np.unique(np.concatenate(xtext))\n",
    "dict_ingredients = dict((ingredients[i],i) for i in range(0, len(ingredients)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may then encode the recipe using a small function that will take an array of ingredients and return a vector of size (number of possible ingredients) with ones at the right places. Eventually we stack all these vectors in our data matrix X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(recipe):\n",
    "    x = np.zeros((1,len(ingredients)))\n",
    "    indices = [dict_ingredients[ing] for ing in recipe]\n",
    "    x[0,indices]=1\n",
    "    return x\n",
    "X = np.vstack([encode(recipes) for recipes in xtext])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is quite voluminous and since our computational ressource are scarses we will remove the columns that corresponds to rarely used ingredients and select only the recipes of 10 types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ingredients=ingredients[np.sum(X,axis=0)>200]\n",
    "selected_type= np.array(['chinese', 'french', 'greek', 'indian', 'italian', 'jamaican',\n",
    "       'korean', 'mexican', 'moroccan', 'thai'])\n",
    "Xs=X[:,np.sum(X,axis=0)>200]\n",
    "Xs=Xs[np.isin(y,selected_type),:]\n",
    "ys=y[np.isin(y,selected_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is now ready to be processed. The final list of ingredients that we will used to recognize the cuisine types is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Shaoxing wine', 'Sriracha', 'active dry yeast',\n",
       "       'all-purpose flour', 'andouille sausage', 'apple cider vinegar',\n",
       "       'arborio rice', 'asparagus', 'avocado', 'baby spinach', 'bacon',\n",
       "       'bacon slices', 'baguette', 'baking potatoes', 'baking powder',\n",
       "       'baking soda', 'balsamic vinegar', 'bananas', 'basil',\n",
       "       'basil leaves', 'basmati rice', 'bay leaf', 'bay leaves',\n",
       "       'beansprouts', 'beef', 'beef broth', 'beer', 'bell pepper',\n",
       "       'black beans', 'black olives', 'black pepper', 'black peppercorns',\n",
       "       'boiling water', 'boneless chicken skinless thigh',\n",
       "       'boneless skinless chicken breast halves',\n",
       "       'boneless skinless chicken breasts', 'bread crumbs', 'broccoli',\n",
       "       'brown rice', 'brown sugar', 'butter', 'buttermilk', 'cabbage',\n",
       "       'cajun seasoning', 'cannellini beans', 'canola oil', 'capers',\n",
       "       'carrots', 'cashew nuts', 'cauliflower', 'cayenne',\n",
       "       'cayenne pepper', 'celery', 'celery ribs', 'cheddar cheese',\n",
       "       'cheese', 'cherry tomatoes', 'chicken', 'chicken breasts',\n",
       "       'chicken broth', 'chicken stock', 'chicken thighs', 'chickpeas',\n",
       "       'chile pepper', 'chiles', 'chili powder',\n",
       "       'chinese five-spice powder', 'chives', 'chopped celery',\n",
       "       'chopped cilantro', 'chopped cilantro fresh',\n",
       "       'chopped fresh chives', 'chopped fresh mint',\n",
       "       'chopped fresh thyme', 'chopped garlic',\n",
       "       'chopped green bell pepper', 'chopped onion', 'chopped parsley',\n",
       "       'chopped pecans', 'chopped tomatoes', 'cider vinegar', 'cilantro',\n",
       "       'cilantro leaves', 'cilantro sprigs', 'cinnamon',\n",
       "       'cinnamon sticks', 'clove', 'coarse salt', 'coconut milk',\n",
       "       'coconut oil', 'cold water', 'confectioners sugar',\n",
       "       'cooked chicken', 'cooked rice', 'cooking oil', 'cooking spray',\n",
       "       'coriander', 'coriander powder', 'coriander seeds', 'corn',\n",
       "       'corn kernels', 'corn starch', 'corn tortillas', 'cornmeal',\n",
       "       'cracked black pepper', 'cream', 'cream cheese',\n",
       "       'creole seasoning', 'crushed red pepper',\n",
       "       'crushed red pepper flakes', 'crushed tomatoes', 'cucumber',\n",
       "       'cumin', 'cumin seed', 'curry leaves', 'curry powder',\n",
       "       'dark brown sugar', 'dark sesame oil', 'dark soy sauce',\n",
       "       'diced onions', 'diced tomatoes', 'dijon mustard', 'dried basil',\n",
       "       'dried oregano', 'dried thyme', 'dry bread crumbs', 'dry red wine',\n",
       "       'dry sherry', 'dry white wine', 'egg whites', 'egg yolks',\n",
       "       'eggplant', 'eggs', 'enchilada sauce', 'evaporated milk',\n",
       "       'extra-virgin olive oil', 'fat free less sodium chicken broth',\n",
       "       'fennel bulb', 'fennel seeds', 'feta cheese',\n",
       "       'feta cheese crumbles', 'fine sea salt', 'finely chopped onion',\n",
       "       'firm tofu', 'fish sauce', 'flank steak', 'flat leaf parsley',\n",
       "       'flour', 'flour tortillas', 'fresh basil', 'fresh basil leaves',\n",
       "       'fresh cilantro', 'fresh coriander', 'fresh dill', 'fresh ginger',\n",
       "       'fresh ginger root', 'fresh lemon juice', 'fresh lime juice',\n",
       "       'fresh mint', 'fresh mushrooms', 'fresh orange juice',\n",
       "       'fresh oregano', 'fresh parmesan cheese', 'fresh parsley',\n",
       "       'fresh rosemary', 'fresh spinach', 'fresh thyme',\n",
       "       'freshly ground pepper', 'frozen chopped spinach', 'frozen peas',\n",
       "       'garam masala', 'garlic', 'garlic cloves', 'garlic paste',\n",
       "       'garlic powder', 'garlic salt', 'ghee', 'ginger', 'golden raisins',\n",
       "       'granulated sugar', 'grape tomatoes', 'grated lemon zest',\n",
       "       'grated nutmeg', 'grated parmesan cheese', 'greek yogurt',\n",
       "       'green beans', 'green bell pepper', 'green cabbage', 'green chile',\n",
       "       'green chilies', 'green onions', 'green peas', 'green pepper',\n",
       "       'ground allspice', 'ground beef', 'ground black pepper',\n",
       "       'ground cardamom', 'ground cinnamon', 'ground cloves',\n",
       "       'ground coriander', 'ground cumin', 'ground ginger',\n",
       "       'ground nutmeg', 'ground pepper', 'ground pork',\n",
       "       'ground red pepper', 'ground turmeric', 'ground white pepper',\n",
       "       'half & half', 'ham', 'heavy cream', 'heavy whipping cream',\n",
       "       'hoisin sauce', 'honey', 'hot pepper sauce', 'hot sauce',\n",
       "       'hot water', 'italian seasoning', 'jalapeno chilies', 'juice',\n",
       "       'ketchup', 'kosher salt', 'large egg whites', 'large egg yolks',\n",
       "       'large eggs', 'large garlic cloves', 'large shrimp',\n",
       "       'lasagna noodles', 'lean ground beef', 'leeks', 'lemon',\n",
       "       'lemon juice', 'lemon wedge', 'lemon zest', 'lemongrass',\n",
       "       'light brown sugar', 'light soy sauce', 'lime', 'lime juice',\n",
       "       'lime wedges', 'linguine', 'long grain white rice',\n",
       "       'long-grain rice', 'low salt chicken broth',\n",
       "       'low sodium chicken broth', 'low sodium soy sauce', 'mango',\n",
       "       'margarine', 'marinara sauce', 'mayonaise', 'medium shrimp',\n",
       "       'melted butter', 'milk', 'minced garlic', 'mint leaves', 'mirin',\n",
       "       'monterey jack', 'mozzarella cheese', 'mushrooms', 'mustard seeds',\n",
       "       'napa cabbage', 'noodles', 'nutmeg', 'oil', 'okra', 'olive oil',\n",
       "       'onion powder', 'onions', 'orange', 'orange juice', 'oregano',\n",
       "       'oyster sauce', 'paprika', 'parmesan cheese',\n",
       "       'parmigiano reggiano cheese', 'parsley',\n",
       "       'part-skim mozzarella cheese', 'pasta', 'pasta sauce', 'peaches',\n",
       "       'peanut oil', 'peanuts', 'peeled fresh ginger', 'pepper',\n",
       "       'pinenuts', 'pitted kalamata olives', 'plain yogurt',\n",
       "       'plum tomatoes', 'pork', 'pork tenderloin', 'potatoes',\n",
       "       'powdered sugar', 'prosciutto', 'purple onion', 'radishes',\n",
       "       'raisins', 'red bell pepper', 'red chili peppers',\n",
       "       'red chili powder', 'red pepper', 'red pepper flakes',\n",
       "       'red potato', 'red wine', 'red wine vinegar', 'refried beans',\n",
       "       'rice', 'rice noodles', 'rice vinegar', 'rice wine',\n",
       "       'ricotta cheese', 'roasted red peppers', 'roma tomatoes',\n",
       "       'romaine lettuce', 'russet potatoes', 'saffron', 'saffron threads',\n",
       "       'sake', 'salsa', 'salt', 'salt and ground black pepper', 'sauce',\n",
       "       'scallions', 'sea salt', 'serrano chile', 'sesame oil',\n",
       "       'sesame seeds', 'shallots', 'shiitake', 'shortening',\n",
       "       'shredded Monterey Jack cheese', 'shredded cheddar cheese',\n",
       "       'shredded mozzarella cheese', 'shrimp', 'sliced green onions',\n",
       "       'sliced mushrooms', 'smoked paprika', 'sour cream', 'soy sauce',\n",
       "       'spaghetti', 'spinach', 'spring onions', 'star anise',\n",
       "       'strawberries', 'sugar', 'sweet onion', 'sweet potatoes',\n",
       "       'sweetened condensed milk', 'taco seasoning', 'thai chile',\n",
       "       'thyme', 'thyme sprigs', 'toasted sesame oil',\n",
       "       'toasted sesame seeds', 'tomatillos', 'tomato paste',\n",
       "       'tomato pur√©e', 'tomato sauce', 'tomatoes', 'tortilla chips',\n",
       "       'tortillas', 'tumeric', 'unsalted butter',\n",
       "       'unsweetened cocoa powder', 'vanilla', 'vanilla extract',\n",
       "       'vegetable broth', 'vegetable oil', 'vegetable oil cooking spray',\n",
       "       'vinegar', 'warm water', 'water', 'whipping cream', 'white onion',\n",
       "       'white pepper', 'white rice', 'white sugar', 'white vinegar',\n",
       "       'white wine', 'white wine vinegar', 'whole milk',\n",
       "       'worcestershire sauce', 'yellow bell pepper', 'yellow corn meal',\n",
       "       'yellow onion', 'yoghurt', 'yukon gold potatoes', 'zucchini'],\n",
       "      dtype='<U71')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the types of cuisines that we must recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chinese', 'french', 'greek', 'indian', 'italian', 'jamaican',\n",
       "       'korean', 'mexican', 'moroccan', 'thai'], dtype='<U8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training and test set\n",
    "\n",
    "As usually, we will split the data into a training and a test dataset. the \"train_test_split\" function from scikit learn is dedicated to this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "We will use randomForest to solve the classification problem as a first try. We will fit a randomforest with \n",
    " trees and estimate the accuracy of such classifier using cross_validation and the test set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7661369505484861"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "scores = cross_val_score(clf, X_train, y_train, scoring='accuracy',cv=5)\n",
    "scores.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also simply fit this model with the training data and produce prediction for the test set with the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute the accuracy with numpy or the built-in function of scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7696131926761246"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = np.sum(y_pred==y_test)/len(y_test)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7696131926761246"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy =accuracy_score(y_test,y_pred)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others metrics can be computed with the classification report tools from scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     chinese       0.77      0.83      0.80       822\n",
      "      french       0.55      0.53      0.54       798\n",
      "       greek       0.71      0.52      0.60       337\n",
      "      indian       0.81      0.85      0.83       864\n",
      "     italian       0.77      0.85      0.81      2399\n",
      "    jamaican       0.78      0.39      0.52       161\n",
      "      korean       0.74      0.54      0.62       232\n",
      "     mexican       0.85      0.88      0.86      1931\n",
      "    moroccan       0.68      0.44      0.54       260\n",
      "        thai       0.83      0.65      0.73       443\n",
      "\n",
      "    accuracy                           0.77      8247\n",
      "   macro avg       0.75      0.65      0.68      8247\n",
      "weighted avg       0.77      0.77      0.76      8247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we may compute the detailled result with the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 679,   17,    2,   16,   30,    2,   33,   18,    0,   25],\n",
       "       [   8,  422,   11,    9,  294,    3,    2,   35,    8,    6],\n",
       "       [   4,   21,  175,   12,   98,    1,    1,   16,    8,    1],\n",
       "       [  15,   15,    3,  735,   24,    1,    2,   49,   16,    4],\n",
       "       [   6,  194,   40,   16, 2038,    2,    0,   89,    9,    5],\n",
       "       [  14,   12,    1,   23,   15,   63,    1,   25,    0,    7],\n",
       "       [  81,    2,    1,    2,    4,    2,  125,    9,    0,    6],\n",
       "       [  17,   59,    4,   25,   97,    5,    1, 1706,   12,    5],\n",
       "       [   3,   17,    8,   51,   39,    0,    0,   27,  115,    0],\n",
       "       [  59,    9,    1,   22,   13,    2,    4,   44,    0,  289]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "### 1) What is the accuracy of this classifier on training data? What do you conclude?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Try to improve the performance of this classifier in terms of accuracy ?\n",
    "You can vary the number of trees between [20,50,100,200]. Use the GridSearchCV function seen in the first notebbok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "hyper_params =[{'n_estimators':[20,50,100,200,300]}]\n",
    "searchres = GridSearchCV(RandomForestClassifier(), hyper_params, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Test another solution (Logistic regression)\n",
    "You will be able to read the documentation of sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Try to implement a rejection solution, to obtain an accuracy of at least 90%? \n",
    "To do this you will use the probabilities provided by the classifier and will only make a decision when these probabilities are above a certain threshold (to be determined). The predict_proba method should be able to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
