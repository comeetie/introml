<!DOCTYPE html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>

@import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz:400,700);
@import url(https://fonts.googleapis.com/css?family=Contrail+One:400,700);

</style>
<link rel="stylesheet" type="text/css" href="library/common.css" />
<link rel="stylesheet" type="text/css" href="library/screen.css" media="screen" />
<link rel="stylesheet" type="text/css" href="library/print.css" media="print" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>

</head>
<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Neural Network </span></h1></br> <h2><p class="grey">UGE,</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>



<section>
<h1 class="green" >ML</h1>
<h2> Problems </h2>
<ul>
<li> Supervised
$$\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_n,y_n)\}\rightarrow\, f(\mathbf{x})\approx y$$
<li> Unsupervised
$$\{\mathbf{x}_1,...,\mathbf{x}_n\}\rightarrow\, f(\mathbf{x}) ?$$
</ul>
</section>
<section>
<h1 class="green" >ML</h1>
<h2> Problems </h2>
But also :
<ul>
<li> Semi-supervised
$$\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_n,y_n)\}+\{\mathbf{x}_1,...,\mathbf{x}_m\}\rightarrow\,f(\mathbf{x})\approx y$$
<li> Self-supervised
$$\{\mathbf{x}_1,...,\mathbf{x}_n\}\rightarrow \{(\mathbf{x}_1,\tilde{y}_1),...,(\mathbf{x}_n,\tilde{y}_n)\}\rightarrow\,f(\mathbf{x})\approx \tilde{y}$$
<li>Re-inforcement learning $$R(a_t,\mathbf{x}_t)$$
</ul>
</section>


<section>
<h1 class="green" >Supervised </h1>
<ul>
<li> Inputs in input feature space $\mathcal{X}$
<li> Outputs in a target space $\mathcal{Y}$</ul>
</ul>
<h2 class="red"> Goal</h2>
find the link function
$$f:\mathcal{X}\rightarrow\mathcal{Y}$$
or in a more probabilistc setting the conditional law $p(y|\mathbf{x})$ between inputs and outputs.
</section>


<section>
<h1 class="green" >Supervised learning </h1>
Hypothesis Space
<ul>
<li> function $f$ belong to a familly of function $\mathcal{H}$ that depends on the methods (SVM, MLP, Decision Tree, Random Forest,...):
$f\in\mathcal{H}$
<li> Goal find the best $f$ in $\mathcal{H}$
</ul>
<h2 class="red"> Goal minimize generalisation error</h2>
$$R(f)=\mathbb{E}_{\mathcal{X},\mathcal{y}}[L(f(\mathbf{x}),y)]=\int_{\mathcal{X}}\int_{\mathcal{y}}L(f(\mathbf{x}),y)p(\mathbf{x},y)d\mathbf{x}dy$$
$L(.,.)$ : loss function that measure the error of a prediction 
</section>


<section>
<h2 class="red">Empirical risk</h2>
A dataset
$S=(\mathbf{x},y):\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_n,y_n)\}$
$$R_S(f)=\frac{1}{n}\sum_{i=1}^nL(f(\mathbf{x}_i),y_i)$$
$$\mathbb{E}[R_S(f)]=R(f)$$
</section>

<section>
<h2 class="red">Empirical risk minimisation</h2>
function $f$ is defined by a weight vector $\color{red}{\mathbf{w}}$, 
$$f(\mathbf{x};\color{red}{\mathbf{w}})$$
find the function $f$ (and therefore the parameters (i.e. the wiegths) $\color{red}{\mathbf{w}}$) which minimize the risk on a dataset
$$\color{red}{\mathbf{\hat{w}}}=\arg\min_{\color{red}{\mathbf{w}}}R_{S_a}(\color{red}{\mathbf{w}})=\arg\min_{\color{red}{\mathbf{w}}}\frac{1}{n}\sum_{i=1}^nL(f(\mathbf{x}_i,\color{red}{\mathbf{w}}),y_i)$$
</section>


<section>
<h1 class="green" >Regularisation</h1>
$$\hat{\mathbf{w}}=\arg\min_{\mathbf{w}}\frac{1}{n}\sum_{i=1}^nL(f(\mathbf{x}_i,\mathbf{w}),y_i)+\color{red}{\lambda}.\color{green}{\Omega(\mathbf{w})}$$

<ul>
<li> $L$ : floss function
<li> $\color{green}{\Omega}$ : regularisation terms mesure the complexity of the model
<li> $\color{red}{\lambda}$ : hyper-parameter that define the amount of regularisation
</ul>

</section>


<section>
<h1 class="green" >Trainning set / testing set</h1>
<h3> double descent</h3>
$f(\mathbf{x})=f(\mathbf{x},\color{red}{\mathbf{w}})$ more parameters 
<img src="./images/doubledescente.png" height=50%>
<span class="red"> ! for some models like  (NN)  + optimization with SGD like algorithm, 2 regimes 
</section>

<section>
<h1 class="green" >Supervised learning in practive</h1>
<h2 class="red"> Data :</h2>  
 $(\mathbf{x},y):\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_n,y_n)\}$ splited in several parts: </br></br>
<ul>
<li> training set ($S_a$, allow to find $f$)
<li> validation set ($S_v$, allow to compare models /architecture and set the hyper-parameters $\mathcal{H}$)
<li> test set ($S_t$, final evaluation of $f$)
</ul>
</section>




<section style="padding-top:5em;text-align:center">
<h1 class="green">NN</h1>
</section>

<section style="padding-top:5em;text-align:center">
<h1 class="green">Neural Networks</h1>
</section>


<section>
<h2> Un peu d'histoire</h2>
<ul>
<li>1943 : Formal neuron (Mc Culloch & Pitts)
<li>1957 : Perceptron (Rosenblatt)
<li>1969 : Limitation of the perceptron (Minsky & Papert)
<li>1974 : Gradient back-propagation (Werbos, pas de difusion) 
<li> 1986 : Gradient back-propagation bis (Rumelhart & McClelland, Lecun)
<ul>
<li> New architecture Convolutional NN
<li> New applications (Character recognition, Speech recognition and synthesis, Vision (image processing) CNN
</ul>
<li> 1997 : LSTM (Hochreiter & Schmidhuber) 
<li> 2005 : Deep networks (Hinton & Salakhutdinov, 2006)
<li> 2010 : auto-diff (Theano, Bengio & al)
<li> 2014 : Generative Adversarial Networks, GAN (Goodfellow & al)
</ul>
</section>

<section>
<h1 class="green" >Formal Neuron ?</h1>

McCulloch, W. S., Pitts, W., A Logical Calculus of the Ideas Immanent in Nervous Activity, Bulletin of Mathematical Biophysics, vol. 5, pp. 115-133, 1943.

$$\mathbb{R}^p\rightarrow\mathbb{R}$$

$$s=\Phi(\mathbf{x}^t\mathbf{w}+b)$$

linear transformation followed by a non linear transform $\Phi$ :
<ul>
  <li> inputs : $[x_1,...x_p]$ </li>
  <li> weights  : $[w_1,...,w_p]$ et biais : $b$ (memory)</li>
  <li> activation function : $\Phi$</li>
  <li> output : $s$ </li>
</ul>

<h3 class="red"> building block of neural network</h3>

</section>

<section >
<h1 class="green" >Formal neuron ?</h1>


$$\Phi(\mathbf{w}^t\mathbf{x}+b)$$

linear transformation followed by a non linear transform $\Phi$ :
<ul>
  <li> hyperbolique tangente $\Phi(x)$ : $tanh(x)$ </li>
  <li> sigmoid $\Phi(x)$ : $1/(1+\exp(- x))$</li>
  <li> relu $\Phi(x)$ : $max(0,x)$</li>
  <li> ...
  <li> softmax (vectorielle) : $\Phi(\mathbf{x})$ : $[\frac{\exp(\mathbf{x}_1)}{\sum_{k=1}^K\exp(\mathbf{x}_k)},...,\frac{\exp(\mathbf{x}_K)}{\sum_k={1^K}\exp(\mathbf{x}_k)}]$ </li>
</ul>
<h3 class="red"> building block of neural network</h3>

</section> 


<section >
<h1 class="green" >Activation</h1>


<h3 class="red"> Hyperbolic Tangeante </h3>
<img src="./images/Activation_tanh.svg" height="50%"></img>

</section> 

<section >
<h1 class="green" >Activation</h1>

<h3 class="red"> Sigmoid </h3><img src="./images/Activation_logistic.svg" height="50%"></img>

</section> 

<section >
<h1 class="green" >Activation</h1>


</h3><h3 class="red"> ReLu </h3>
<img src="./images/Activation_rectified_linear.svg" height="50%"></img>

</section> 



<section >

<h1 class="green" >MLP</h1>
Multi-layer perceptron. neurones are arranged in layers from <span class="red">inputs $\mathbf{x}$</span> to <span class="green">outputs $\mathbf{y}$</span>
<img src="./images/Colored_neural_network.svg" height="40%"></img>
Network Weights: $\mathbf{W} = [(\mathbf{w}^1,b^1)...,(\mathbf{w}^M,b^m)]$
</section>

<section >
<h1 class="green" >MLP</h1>
Multi-layer perceptron. neurones are arranged in layers from <span class="red">inputs $\mathbf{x}$</span> to <span class="green">outputs $\mathbf{y}$</span>
<img src="./images/Colored_neural_network.svg" height="40%"></img>
$$G(\mathbf{x},\mathbf{W})_r=\Phi^2(b_r^2+\sum_{q=1}^Qw_{rq}^2\Phi^1(b_q^1+\sum_{p=1}^Pw_{qp}^1x_{p}))$$
</section>


<section >
<h1 class="green" >MLP</h1>
Multi-layer perceptron. neurones are arranged in layers from <span class="red">inputs $\mathbf{x}$</span> to <span class="green">outputs $\mathbf{y}$</span>
<img src="./images/Colored_neural_network.svg" height="40%"></img>
$$G(\mathbf{x},\mathbf{W})=\Phi^2(\mathbf{W}^2\Phi^1(\mathbf{W}^1x))$$
</section>

<section >
<h1 class="green" >MLP</h1>
Multi-layer perceptron. neurones are arranged in layers from <span class="red">inputs $\mathbf{x}$</span> to <span class="green">outputs $\mathbf{y}$</span>
<img src="./images/Colored_neural_network_2.svg" height="40%"></img>
$$G(\mathbf{x},\mathbf{W})_r=\Phi^3(b_r^3+\sum_{q=1}^Qw_{rq}^3\Phi^2(b_r^2+\sum_{q=1}^Qw_{rq}^2\Phi^1(b_q^1+\sum_{p=1}^Pw_{qp}^1x_{p})))$$
</section>

<section >
<h1 class="green" >MLP</h1>
Multi-layer perceptron. neurones are arranged in layers from <span class="red">inputs $\mathbf{x}$</span> to <span class="green">outputs $\mathbf{y}$</span>
<img src="./images/Colored_neural_network_2.svg" height="40%"></img>
$$G(\mathbf{x},\mathbf{W})=\Phi^3(\mathbf{W}^3\Phi^2(\mathbf{W}^2\Phi^1(\mathbf{W}^1x)))$$
</section>



<section >
<h1 class="green" >MLP</h1>

<span class="red">Architecture :</span>
<ul>
<li> Number of layer 
<li> Width of the layers
<li> Activation function
<li> Regularisation
<li> <span class="red">Loss function </span>
<ul>
<li> Regression : mse $L=\sum_{i=1}^N||G(\mathbf{x}_i)-y_i||^2$, mae $L=\sum_{i=1}^N|G(\mathbf{x}_i)-y_i|$ 
<li> Classification : cce $L=-\sum_{i=1}^N(y_i\log(G(\mathbf{x}_i))+(1-y_i)log(1-G(\mathbf{x}_i)))$
</ul>
<li>Last layer
<ul>
<li> Regression : linear
<li> Classification : sigmoid (binary), soft-max (mulitclass) 
<ul>
</section>

<section >

<h1 class="green" >MLP</h1>
<h3 class="red">Learning</h3>
<ul>
<li> weights  optimization to minimize $L(G(\mathbf{x},\mathbf{w}),y))$ for a given training set $\{(x_1,y_1),...,(x_n,y_n)\}$
<li> Stochastic gradient descent
<li> gradients $[\frac{\delta L(\mathbf{w})}{\delta W_{11}}, ..., \frac{\delta L(\mathbf{w})}{\delta W_{ld}}]$ can be efficiently computed thanks to the chain rule : 
$$f(g(x))'=f'(g(x))g'(x)$$
i.e. Back-propagation
</ul>
</section>

<section >
<h3>Gradient descent</h3>
update formula :
$$\mathbf{w} \leftarrow \mathbf{w}-\color{red}\lambda\frac{\delta L(\mathbf{w})}{\delta \mathbf{w}}$$
<img src="./images/gradient.png" height="65%"></img>
</section>


<section >

<h3>Gradient descent</h3>
update formula :

$$\mathbf{w} \leftarrow \mathbf{w}-\color{red}\lambda\frac{\delta L(\mathbf{w})}{\delta \mathbf{w}}$$
<ul>
<li> compute the gradient $\frac{\delta L(\mathbf{w})}{\delta \mathbf{w}}$
<li> update the weights by performing a step of size  $\color{red}{\lambda}$
<li> in the opposite direction of the gradient 
<li> // steepest descent direction
<li> iterate until convergence
<ul>

if $\lambda$ small enough will converge to a minima
</section>



<section >


<h3>Gradient descent</h3>
update formula :

Initialize $\mathbf{w}_1$ et $\lambda=0.001$, $\alpha=0.1,L_{old}=-\infty$</br></br>
while $|L(\mathbf{w}_{n})-L_{old}|>\epsilon$:
<ul>
<li> update the wieghts :
$$\mathbf{w}_{n+1} \leftarrow \mathbf{w}_n-\color{red}\lambda\frac{\delta L(\mathbf{w}_n)}{\delta \mathbf{w}}$$
<li> $L_{old} \leftarrow L(\mathbf{w}_n)$
<li> decreasing test if $L(\mathbf{w}_{n+1}) > L_{old}$
<ul>
<li> step size reduction $\lambda \leftarrow\lambda\alpha$ 
</ul>
<li> $n\leftarrow n+1$
<ul>
</section>


<section>
<img src="images/grad1.png" width="60%"></img>
Convexe problem
</section>

<section>
<img src="images/grad2.png" width="60%"></img>
Non convex problem $\rightarrow$ local minima
</section>


<section>
<img src="images/grad3.png" width="60%"></img>
convex problem
</section>

<section>
<img src="images/grad4.png" width="60%"></img>
Non convex problem $\rightarrow$ local minima (final solution initialization dependant)
</section>

<section>
<img src="images/grad5.png" width="60%"></img>
Non convex problem $\rightarrow$ local minima (final solution initialization dependant)
</section>

<section >

<h1>Stochastic gradient descent</h1>
$$\frac{1}{\color{green}{n_b}}\sum_{i \in \color{green}{\mathbb{B}_j}} L(G(\mathbf{x}_i,\mathbf{w}),y_i) \approx \frac{1}{N} \sum_{i=1}^N L(G(\mathbf{x}_i,\mathbf{w}),y_i)$$

<ul>
<li> Perform $m$ epochs (a complet scan of the training set):   
<li> shuffle the training set
<li> split in mini batch (mini-batch $\color{green}{\mathbb{B}_j}$) of size $\color{green}{n_b}$
<li> for each mini-batch update the weights with the gradients computed on the mini-batch 

</ul>
</br>
<span class="red">Online gradient, mini-batch of size $n_b=1$</span></br>
<span class="red">$\Rightarrow$Small complexity, works with huge datasets </span></br>

</section>

<section>
<h1>Momentum</h1>
<span class="red">$\Rightarrow$Variantes that take into acount the dynamic of the updates (add some intertia in the updates):</br>
 rmsprop, Adam,...</span>
$$
\begin{align}
\mathbf{z}_{n+1} &= \beta \mathbf{z}_{n} + \frac{\delta L(\mathbf{w})}{\delta \mathbf{w}}\nonumber\\
\mathbf{w}_{n+1} &= \mathbf{w}_{n} -\lambda \mathbf{z}_{n+1}\nonumber
\end{align}
$$

<ul>
<li><a href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a>
<li><a href="https://ruder.io/optimizing-gradient-descent/">https://ruder.io/optimizing-gradient-descent/</a>
</ul>
</section>



<section >
<h3>Neural Network Libraries</h3>
<h2 class="red">Back Propagation -> automatic differentiation</h2>
Reverse mode automatic differentiation
Define the network architectecture, the libraries will automaticaly create function to compute the gradients 
<ul>
<li> Theano
<li> Torch
<li> TensorFlow
<li> ...
<ul> 
</section>







<section>
<h1 class="green">Keras</h1>
<h1><a href="https://keras.io/">https://keras.io/</a></h1>
</section>

<section >
<h1>Keras</h1>
</br>
high level librairy to define nn and learn their parameters </br>
//Sequential interface
<pre><code class="python">model = Sequential()
model.add(Dense(32, activation='relu', input_dim=100))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Generate dummy data
import numpy as np
data = np.random.random((1000, 100))
labels = np.random.randint(10, size=(1000, 1))

# Convert labels to categorical one-hot encoding
one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)

# Train the model, iterating on the data in batches of 32 samples
model.fit(data, one_hot_labels, epochs=10, batch_size=32)
</pre></code>
</section>

<section >
<h1>Keras</h1>
high level librairy to define nn and learn their parameters </br>
//Functional Interface 
<pre><code class="python">from keras.layers import Input, Dense
from keras.models import Model
# This returns a tensor
inputs = Input(shape=(784,))
# a layer instance is callable on a tensor, and returns a tensor
h1 = Dense(64, activation='relu')(inputs)
h2 = Dense(64, activation='relu')(h1)
predictions = Dense(10, activation='softmax')(h2)
# This creates a model that includes
# the Input layer and three Dense layers
model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='rmsprop',loss='categorical_crossentropy')
model.fit(data, labels)  # starts training
</pre></code>
</section>


<section>
<img src="./images/tensorplay.png" width="85%">
<a href="http://playground.tensorflow.org">http://playground.tensorflow.org</a>
</section>




<section >

<h1>Hyper parameters</h1>
Réglage des hyperparamètres :
<h4 class="green">Architecture :</h4>
<ul>
<li> number of layer
<li> layer width
<li> activations 
<li> regularisation
</ul>
<h4 class="green"> Optimisation : </h4>
<ul>
<li> mini-batchs size
<li> learning rates
<li> number of epochs
</ul>
 </br>validation set
</section>

<section>
<h1>Batch norm</h1>
$$
s = \frac{s-m_s}{\sqrt{\sigma_s^2+\epsilon}}
$$
<ul>
<li> control mean and variance between layer
<li> each input is controlled independantly
<li> during training : mean and variance of thebatch
<li> at testing : (mean variance) over entire train set 
<ul>
</section>


<section>
<h1>deep networks</h1>
 <span class="red">! optimisation (vanishing gradient)</span>
<h2 class="green">Relu</h2>
<img src="./images/Activation_rectified_linear.svg" height="30%">
<h2> + de gradient </h2>
</section>


<section>
<h1>Regularisation noise injection</h1>
<span class="red">Drop out</span>
<ul>
<li> During trainig disconnect randomly some weight with a probability $p$.
<li> at testing, multiply the weightby their observed disconnection probability 
</ul>
<img src="./images/drop_out1.png" width="30%">
<span class="small">Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of
feature detectors. arXiv:1207.0580.</span>
</section>

<section>
<h1>Regularisation, noise injection</h1>
<span class="red">Drop out</span>
<ul>
<li> During trainig disconnect randomly some weight with a probability $p$.
<li> at testing, multiply the weightby their observed disconnection probability 
</ul>
<img src="./images/drop_out2.png" width="30%">
<span class="small">Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of
feature detectors. arXiv:1207.0580.</span>
</section>




<section style="padding-top:5em;text-align:center">
<h1>images, texte, séquences, série temporelle</h1>
</section>


<section>
<h1>Modern networks</h1>
<h4>Modular architecture</h4>
may handle different data </br>
<ul>
<li> Denses layer (MLP) 
<li> Convolutional layer (Images)
<li> Recurrent layers (Sequences, temps)
</ul>
<h4>Combinaation ?</h4>
<ul>
<li> Stack different layers 
<li> Genreci optimisation algorithm (Reverse Mode AD // BP) + SGD variants
</ul>
</section>
<section style="padding-top:5em;text-align:center">
<h1>CNN</h1>
</section>

<section >
<h1>Images</h1>
$\Rightarrow$ convolution, pooling, (translation invariance)
<div style="text-align:center;height:80%" >
<table style="margin:auto;height:80%">
<tr><td>$\begin{bmatrix}0 & 1 & 0\\1 & -4 & 1\\0 & 1 & 0 \end{bmatrix}$</td>
<td><img src="./images/generic-taj-convmatrix-edge-detect.jpg" height="95%"></img></td></tr>
</table>
</div>
</section>

<section>
<h1>Convolution</h1>
$\Rightarrow$ convolution, pooling, (translation invariance)
<img src="./images/convlayer.gif" height="70%"></img>
</section>

<section>
<h1>Convolution</h1>
$\Rightarrow$ convolution, pooling, (translation invariance)
<img src="./images/conv2layer.gif" height="70%"></img>
</section>

<section>
<h1>Convolution</h1>
$\Rightarrow$ convolution, pooling, (translation invariance)
<img src="./images/cnn.png" height="70%"></img>
</section>


<section>
<h1>Max pooling</h1>
Compression
<img src="./images/maxpool.gif" width="60%"></img>
</section>


<section >
<h1>Images</h1>
$\Rightarrow$ convolution, pooling, (translation invariance)
<img src="./images/convnet.png" width="90%"></img>


<span class="small">LeCun, Y. (1989). Generalization and network design strategies. Connections in Perspective. North-Holland, Amsterdam, 143-55.</br>
LeCun, Y., Kavukcuoglu, K., & Farabet, C. (2010, May). Convolutional networks and applications in vision. In Circuits and Systems (ISCAS),
Proceedings of 2010 IEEE International Symposium on (pp. 253-256). IEEE.</span>
</section>


<section >
<h1>Images</h1>
$\Rightarrow$ convolution, pooling, (translation invariance)
<img src="./images/cnnlecun.png" height="40%"></img>
<span class="small">LeCun, Y. (1989). Generalization and network design strategies. Connections in Perspective. North-Holland, Amsterdam, 143-55.</br>
LeCun, Y., Kavukcuoglu, K., & Farabet, C. (2010, May). Convolutional networks and applications in vision. In Circuits and Systems (ISCAS),
Proceedings of 2010 IEEE International Symposium on (pp. 253-256). IEEE.</span>
</section>

<section>
<h1 class="red">CNN, Architecture type</h1>
Image $\rightarrow\{1,...,K\}$
<ul>
<li> Convolutional blocks (n times)
<ul>
<li>Convolutional layer
<li>Batch normalization
<li>Relu activation
<li>(Subsampling)
</ul>
<li>Dense blocks (n times)
<ul>
<li>Batch normalization
<li>Dense layer (+ weights regularization)
<li>Relu/Tanh activation
</ul>
<li>Final layer
<ul>
<li>Batch normalization + Dense + Softmax activation
</ul>
</ul>
</section>


<section >
<h1>Sequences,/h1>
<h3>Recurrent neural nets(RNN)</h3>
LSTM, GRU :</br>
<img src="./images/RNN-unrolled.png" width="50%" style="margin-top:1em">
unfolded // classical NN with shared weights
</section>


<section>
<h1>RNN</h1>
<img src="images/rnn/architecture-rnn.png" width="50%"></img>
$$a_t = \Phi(W_{aa}a_{t-1}+W_{ax}x_{t}+b_a)$$
$$y_t = \Phi(W_{ya}a_{t}+b_y)$$
</section>




<section>
<h1>Transfer learning</h1>
<ul>
<li>Use the first layer of a network learnt on an important dataset
<li>"Finetuning" of the last layers on the target task. 
</ul>
<img src="./images/transfert.png" width="40%">
</section>

<section >
<h1>Self supervised learning</h1>
<ul>
<li> Learn with a pretext task derived from unsupervised data</br>
<span class="red">! (no labels but supervied) </span>
<li>Use as first layer in final architecture 
</ul>
<img src="./images/pretext_selfsup2.png" height="60%">
</section>

<section >
<h1>Self supervised learning</h1>
<ul>
<li> Learn with a pretext task derived from unsupervised data</br>
<span class="red">! (no labels but supervied) </span>
<li>Use as first layer in final architecture 
</ul>
<img src="./images/colorisation.png" height="60%">
</section>




<section >
<h1>Self supervised learning</h1>
<ul>
<li> Learn with a pretext task derived from unsupervised data</br>
<span class="red">! (no labels but supervied) </span>
<li>Use as first layers in another model
</ul>
<img src="./images/pretext_selfsup.png" height="60%">
</section>

<section >
<h1>Self supervised learning</h1>
<ul>
<li> Learn with a pretext task derived from unsupervised data</br>
<span class="red">! (no labels but supervied) </span>
<li>Use as first layers in another model
</ul>
<img src="./images/patch.png" height="60%">
</section>




<section >
<h1>Texte, WordEmbedding</h1>
<img src="./images/wordembedding.png" height="75%" style="margin-top:1em">
</section>

<section>
<h1>Texte, WordEmbedding</h1>
Predict next word

<img src="./images/nn_language_model-1.jpg" height="55%" style="margin-top:1em">
Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. Retrieved from http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf ↩︎
</section>

<section>
<h1>Texte, WordEmbedding</h1>
Prédire le prochain mots:

<ul>
<li> <b>Embedding Layer</b>: produit le word embeding index vector . word embeding matrix</br>
<li> <b>Intermediate Layer(s)</b>: consrtuction de représentation inetrmédiaires, e.g. a couches denses prenant en entrée la concatenation des embeddings des mots précedents;</br>
<li> <b>Softmax Layer</b>: couche de décision produit une probabilité sur le vocabulaire V</br>
</ul>
</section>




<section>
<h1>Texte, WordEmbedding, CBOW</h1>
predict a word from its context :

<img src="./images/cbow.png" height="55%" style="margin-top:1em">
Mikolov, T., Corrado, G., Chen, K., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. ↩︎

</section>

<section>

<h1>Texte, WordEmbedding, CBOW</h1>
predict a word from its context :<ul>
<li> <b>Embedding Layer</b>:: index vector . word embeding matrix;
<li> <b>Intermediate Layer(s)</b>: a simple mean of the context words embeddings 
<li> <b>Softmax Layer</b>: decision layer distribution over vocabulary V
</ul>
</section>

<section>
<h1>Texte, WordEmbedding, CBOW</h1>
The last layer is costly (|V| is big)
$$p(w|c)=\frac{\exp(h^{t}v_{w})}{\sum_{w_i\in V}\exp(h^{t}v_{w_{i}})}$$
Change a little bit the pretext task
-> link with "Contrastive learning" / work with pairs of words
</section>


</section>
<section>
<h1>Texte CBOW + Negative sampling</h1>
<h3>Pretexte task : close words // Random words</h3>
$\Rightarrow$ one-hot encoding, of the words  $(0,0,...,1,...,0,0)$</br>
position in the dictionnary

<h4>Example wordEmbeding :</h4> 
<ul>
<li> $\mathbf{z}_1 = \mathbf{x}_1^t\mathbf{V}$
<li> $\mathbf{z}_2 = \mathbf{x}_2^t\mathbf{V}$
<li> $s = \frac{\mathbf{z}_1^t\mathbf{z}_2}{||\mathbf{z}_1||||\mathbf{z}_2||}$
<li> $y = sigmoid(s)$
</ul>
</br>
<h4>Training set :</h4>
<ul>
<li>pairs of words $(\mathbf{x}_1,\mathbf{x}_2)$ that we observe in a small window ($y=1$)
<li>pairs of random words $(\mathbf{x}_1,\mathbf{x}_2)$  ($y=0$)
</ul>
$\Rightarrow \mathbf{V}$ embedding each words is now associated with a $d$ dimensional vector that can be used in place of the words for other task.  
</section>


<section>
<h1>Texte CBOW + Negative sampling</h1>

<h3>Tâche pretexte : Mots proche // Mots distants</h3>

<h4>Training set building</h4>
<ul>
<li> How many negative sample per positive sample <i>(nb_neg)</i>
<li> How to draw random words (smoothed unigram distribution, $\alpha$) 
$$ps_{w_i}^\alpha= \frac{\#w_i^{\alpha}}{\sum_j\#w_j^\alpha}$$
<li> Sub sampling of common words ($t$)
$$ pr_{w_i} = 1 -\sqrt(\frac{t}{ps_{w_i}^1})$$
</ul>
</section>



<section>
<h1> Links and ressources </h1>

& images credits 

<ul>
<li><a href="https://www.tensorflow.org/tutorials">https://www.tensorflow.org/tutorials</a>

<li> <a href="https://stanford.edu/~shervine/teaching/cs-230/"> standford cs230 </a>

<li> <a href="https://github.com/rasbt/stat453-deep-learning-ss20"> stat 453 </a>

<li> <a href="https://atcold.github.io/pytorch-Deep-Learning/"> course yann lecun </a>
 
<li> <a href="https://moodle.insa-rouen.fr/course/view.php?id=1207">romain herault insa </a>

<li><a href="https://drive.google.com/file/d/1e_9W8q9PL20iqOR-pfK89eILc_VtYaw1/view"> Gradient-based Optimization in deep learning </a>



</ul>


</section>




<script src="./library/d3.v3.min.js"></script>
<script src="./library/stack.v1.min.js"></script>
<link rel="stylesheet" href="./library/styles/hybrid.css">
<script src="./library/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script>

var mystack = stack()
    .on("activate", activate)
    .on("deactivate", deactivate);

var section = d3.selectAll("section"),
    follow = d3.select("#follow"),
    followAnchor = d3.select("#follow-anchor"),
    lorenz = d3.select("#lorenz"),
    followIndex = section[0].indexOf(follow.node()),
    lorenzIndex = section[0].indexOf(lorenz.node());

function refollow() {
  followAnchor.style("top", (followIndex + (1 - mystack.scrollRatio()) / 2 - d3.event.offset) * 100 + "%");
}

function activate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", refollow);
  if (i === lorenzIndex) startLorenz();
}

function deactivate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", null);
  if (i === lorenzIndex) stopLorenz();
}


</script>
