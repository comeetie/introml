<!DOCTYPE html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>

@import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz:400,700);
@import url(https://fonts.googleapis.com/css?family=Contrail+One:400,700);

</style>
<link rel="stylesheet" type="text/css" href="../../library/common.css" />
<link rel="stylesheet" type="text/css" href="../../library/screen.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../../library/print.css" media="print" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>00
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>

</head>
<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Introduction to DS/ML  </span></h1></br> <h2><p class="grey">Master, UGE</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>



<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Supervised learning</span></h1></br> <h2><p class="grey">Master, UGE</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>

<section style="background-color:white;color:black">

<h1 class="green" >Classification ?</h1>


<ul>
<li> Apprentissage supervisé
<li> Data : 
</ul>
Données d'apprentissage :
$$\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_N,y_N)\}, \mathbf{x_i} \in \mathcal{R}^p,\,y_i \in \{G_1,...,G_K\}$$

<h3 class="red">Objectif : </h3>

Trouver $f$ :  $f(x) \sim y$ pour prédire $y$ avec de nouveaux $x$</br>
<span class="green">Exemple : </span>  e-mail :  spam / non spam contenu + meta-data
</section>

<section style="background-color:white;color:black">
<h1 class="green" >Classification ?</h1>
<ul>
<li> $\neq$ Regression ou $y$ est un réel
</ul>
Data :
$$\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_N,y_N)\}, \mathbf{x_i} \in \mathcal{R}^p,\,y_i \in \mathbb{R}$$

<h3 class="red">Objectif : </h3>

Trouver $f$ :  $f(x) \sim y$ pour prédire $y$ avec de nouveaux $x$</br>
<span class="green">Exemple : </span> y=consomation electrique, x=information sur le ménage, la maison, la météo, le calendrié
</section>

<section style="background-color:white;color:black">
<h1>Regression</h1>
<img src="./images/kernel_ridge.png" height="65%">
</section>

<section style="background-color:white;color:black">
<h1>Classification</h1>
<img src="./images/ex_classif.png" height="65%">
</section>

<section style="background-color:white;color:black">
<h1>Classification, for prediction</h1>
<img src="./images/ex_classif_test.png" height="65%">
</section>

<section style="background-color:white;color:black">
<h1 class="green" >Apprentissage supervisé</h1>
Espace des hypothèses:
<ul>
<li> $f$ appartient a une famille de fonction $\mathcal{H}$ qui dépend de la méthode et d'hyper-paramètre (SVM, MLP, Decision Tree, Random Forest,...):
$f\in\mathcal{H}$
<li> trouver la meilleure fonction $f$ dans $\mathcal{H}$
</ul>
<h2 class="red"> La meilleure ? Erreur de généralisation</h2>
$$R(f)=\mathbb{E}_{\mathcal{X},\mathcal{y}}[L(f(\mathbf{x}),y)]=\int_{\mathcal{X}}\int_{\mathcal{y}}L(f(\mathbf{x}),y)p(\mathbf{x},y)d\mathbf{x}dy$$
$L(.,.)$ : fonction de cout i.e "loss" $f$ 
</section>


<section style="background-color:white;color:black">
<h2 class="red">Regression, loss fonction :</h2>
$\mathcal{Y}=\mathbb{R}$, mean square error :
$L(f(\mathbf{x},y)=(f(\mathbf{x})-y)^2$
<img src="./images/kernel_ridge.png" height="55%" style="margin-top:1em">
<span class="green">Exemple : </span> y=electrical consumption, x=houseold information, house description, meteo,calendar information
</section>

<section style="background-color:white;color:black">

<h2 class="red">Classification, loss function :</h2>
$\mathcal{Y}=\{0,1\}$ (discret set of outcomes), classification error :
$L(f(\mathbf{x}),y)=\mathbf{1}_{f(\mathbf{x})\neq y}$
<img src="./images/ex_classif.png" height="55%" style="margin-top:1em">
</br>
Ex: $y:\{spam, notspam\}$, $\mathbf{x}$ : e-mail data
</section>

<section style="background-color:white;color:black">
<h2 class="red">Risque empirique</h2>
$S=(\mathbf{x},y):\{(\mathbf{x}_1,y_1),...,(\mathbf{x}_n,y_n)\}$
$$R_S(f)=\frac{1}{n}\sum_{i=1}^nL(f(\mathbf{x}_i),y_i)$$
$$\mathbb{E}[R_S(f)]=R(f)$$
</section>

<section style="background-color:white;color:black">
<h2 class="red">Minimisation du risque empirique</h2>
$f$ définie par un vecteur de paramètres $\color{red}{\mathbf{w}}$, 
$$f(\mathbf{x};\color{red}{\mathbf{w}})$$
Trouver $f$ (i.e trouver $\color{red}{\mathbf{w}}$) qui minimise le risque empirique 
$$\color{red}{\mathbf{\hat{w}}}=\arg\min_{\color{red}{\mathbf{w}}}R_{S_a}(\color{red}{\mathbf{w}})=\arg\min_{\color{red}{\mathbf{w}}}\frac{1}{n}\sum_{i=1}^nL(f(\mathbf{x}_i,\color{red}{\mathbf{w}}),y_i)$$
</section>

<section style="background-color:white;color:black">
<h2 class="red">Exemple regression</h2>
$\mathcal{H}: f(\mathbf{x})= \mathbf{x}^Tw = \mathbf{x}^1w_1+...+\mathbf{x}^pw_p$ i.e fonctions linéaires, </br>
Loss : mse, risque empirique :
$$\sum_{i=1}^n(\mathbf{x}_i^Tw-y_i)^2$$
$$\hat{w} = (X^TX)^{-1}(X^Ty)$$
</section>


<section style="background-color:white;color:black">
<h2 class="red">Exemple regression</h2>
$\mathcal{H}: f(\mathbf{x})= [\mathbf{x},\mathbf{x}^2]^Tw$  i.e fonctions quadratiques, </br>
L
Loss : mse, risque empirique :
$$\sum_{i=1}^n(\mathbf{x}_i^Tw-y_i)^2$$
$$\hat{w} = (X^TX)^{-1}(X^Ty)$$
</section>


<section style="background-color:white;color:black">
<h1 class="green" ></h1>
<h3> Training set / Test set</h3>
$f(\mathbf{x})=f(\mathbf{x},\color{red}{\mathbf{w}})$ plus ou moins complexe (controler la complexité)
<img src="./images/biais_variance.png" height=50%>
<span class="red"> ! nous cherchons une fonction qui va bien fonctionner sur de nouveaux exemples
</section>


<section style="background-color:white;color:black">
<h1 class="green" >Classification supervisé</h1>
<h3> Trainning data / Test data</h3>
<ul>
<li> estimation de l'erreur de généralisation $\sim 70\%$ (Training), $\sim 30\%$ (Test)
<li> 
</ul>
<h3 class="red">Cross validation: </h3>
<ul> 
<li> découper en $V$ group.
<li> pour chaque groupe calculer le risque empirique du modèle appris avec les autres groupes 
<li> moyénner les erreurs
</ul>
</section>



<section style="background-color:white;color:black">

<h1 class="green" >Classification, </h1>
performances de $f$
<h4>Matrice de confusion</h4>
$$C_{kl}\, = \#(y_i=c_k \cap \hat{f}(\mathbf{x}_i)=c_l)$$
<img src="./images/confusion.png" height="50%">
</section>


<section style="background-color:white;color:black">

<h1 class="green" >Classification,</h1>
 performances de $f$
<h4>Matrice de confusion</h4>
$$C_{kl}\, = \#(y_i=c_k \cap \hat{f}(\mathbf{x}_i)=c_l)$$
<img src="./images/contable.png" height="50%">
https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers
</section>




<section style="padding-top:5em">
<h1>Quelques Classifieurs</h1>
</section>




<section style="padding-top:5em;text-align:center">
<h1>Regression logistic</h1>
</section>

<section style="background-color:white;color:black">
<h1 class="blue">Regression logistic</h1>
$$P(y=c_k|\mathbf{x})=\frac{exp(\beta_k^{t}\mathbf{x})}{\sum_{h=1}^{K}exp(\beta_h^{t}\mathbf{x})}$$
<ul>
<li>Apprentissage = trouver les $\beta$ (maximum de vraissemblance)
$$\hat{\beta_1},...,\hat{\beta}_k=\arg\max \prod_{i=1}^{N}\prod_{k=1}^{K}P(y=c_k|\mathbf{x_i})^{1_{y_i=G_k}}$$
</ul>
</section>

<section style="background-color:white;color:black">
<h1 class="blue">Regression logistic</h1>
$$P(y=c_k|\mathbf{x})=\frac{exp(\beta_k^{t}\mathbf{x})}{\sum_{h=1}^{K}exp(\beta_h^{t}\mathbf{x})}$$
<ul>
<li>Apprentissage = trouver les $\beta$ 
<li>loss function = categorical cross entropy = $−(y\log(p)+(1−y)\log(1−p))$
<li>problème d'optimisation (convexe)
<li>Frontière de décision linéaires
</ul>
</section>


<section style="padding-top:5em;text-align:center">
<h1>K nearest neighbours Kppv</h1>
</section>

<section style="background-color:white;color:black">
<h1 class="blue">K nearest neighbours K-ppv</h1>
Pour prédire un nouveau point $\mathbf{x}$:
<ul>
<li> Trouver les k exemple $\{i_1,...,i_k\}$ les plus proches de $\mathbf{x}$ dans les données d'apprentissage
<li> Vote majoritaire pour obtennir une prédiction
</ul>
<h3>Remarque</h3>
<ul>
<li>Hyper-Parametre $k$, 
<li>Apprentissage = "mémorisation du jeu de données"
<li>! cout algorithmique lors de la prédiction
<li> attention au claul de distance ! i.e centrage réduction
</ul>
</section>



<section style="padding-top:5em;text-align:center">
<h1>Support</h1>
<h1>Vector</h1>
<h1>Machine</h1>
</section>

<section style="padding-top:5em;text-align:center">
<h1>S</h1>
<h1>V</h1>
<h1>M</h1>
</section>
<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Frontière de décision optimale</h3>
cas séparable
<img src="images/svm1.png" height="60%">
</section>


<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Frontière de décision optimale</h3>
cas séparable
<img src="images/svm2.png" height="60%">
</section>

<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Frontière de décision optimale</h3>
cas séparable
<img src="images/svm3.png" height="60%">
</section>

<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Frontière de décision optimale</h3>
cas séparable
<img src="images/svm4.png" height="60%">
</section>

<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Frontière de décision optimale</h3>
cas séparable
<img src="images/svm4.png" height="30%">
avec $y_i\in\{-1,1\}$ = binary case
$$\hat{w},\hat{b} = \arg\min_{w}||w||$$
avec $y_i(w^t\mathbf{w}+b)-1>0,\,\forall i \in\{1,...,N\}$
</section>


<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Frontière de décision optimale</h3>
cas non séparable
<img src="images/svm5.png" height="60%">
</section>



<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Frontière de décision optimale</h3>
cas non séparable
<img src="images/svm5.png" height="30%">
$$\hat{w},\hat{b} = \arg\min_{w}||w||+C\sum_{i=1}^{N}max(0,1-y_i(w^t\mathbf{w}+b))$$
</section>

<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Remarques </h3>
<ul>
<li> Problème convexe
<li> peut être "Kerneliser" = ré-écrit avec uniquement des produits scalaires
<li> solution parcimonieuse qui utilise uniquement quelques point 
<h3 class="purple">"vecteurs supports"</h3>
</ul>
</section>





<section style="background-color:white;color:black">
<h1 class="blue">SVM</h1>
<h3> Kernel/ Noyau</h3>
<ul>
<li> Kernel = produit scalaire dans un autre espace
  $$K(\mathbf{x}_i,\mathbf{x}_j)=<\phi(\mathbf{x}_i),\phi(\mathbf{x}_j)>$$
<li> transformation non linéaire 
<li> kernel classique = noyau gaussian </br>
(equivalent a un produit scalaire dans  un espace infini ;)
</ul>
<img src="./images/ex_kernel.png" height="30%">
</section>

<section style="padding-top:5em;text-align:center">


<h1>Decision</h1>
<h1>Tree</h1>
</section>


<section style="background-color:white;color:black">
<h1 class="blue">Decision Tree</h1>
<img src="./images/Arbre_de_decision.jpg" height="50%">
</section>

<section style="background-color:white;color:black">
<h1 class="blue">Arbre de décision</h1>
<ul>
<li> recherche gloutone de split mono-dimensionel </br>
<p style="text-align:center">= "information gain -> faire décroitre l'entropie"</p>
$$H(p)=-p\log(p)-(1-p)\log(1-p)$$
$$G = H_{before}-(p_{right}.H_{right}+p_{left}.H_{left})$$
<li> jusqu'a ce que toute les feuille soient "pures" (H=0)
</ul>
Remarques:
<ul>
<li> split = partionnenement rectangulaire de l'espace
<li> ! over-fitting -> prunning (suppression de noeud = simplification )
  <li> prediction : vote majortaire dans la feuille
</ul>
</section>

<section style="background-color:white;color:black">

<h1>Random forest</h1>
<img src="./images/randomForest1.png" height="70%">
</section>

<section style="background-color:white;color:black">

<h1>Random forest</h1>
<ul>
<li> construire un ensemble d'arbre (petits avec peu de #splits)</br><span class="red"> ! aussi different que possible (perturber les données)</span></br>
et les combiner</br>
Exemple : <ul>
<li>sous échantilloner les données 
<li>sous échantilloner les variables
</ul>
<li> Combinaison  <span class="red">vote majoritaires</span>
<li> Controler l'over fitting
</ul>
</section>



<section style="background-color:white;color:black">
<h1 class="blue"></h1>
<h3> with scikit-lean </h3>

clf = RandomForestClassifier(n_estimators=20)</br>
scores = cross_val_score(clf, X_train, y_train, scoring='f1_weighted')</br>
scores.mean()  </br>
</section>




<section>
	<h1>Ressources</h1>


<ul>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/"> Element of statistical Learning </a>
	<li> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf"> Pattern recognition and Machine Learning</a>
</ul>


</section>

<section>

<h1 class="red"> ! Dangers ! </h1>

<ul> 
<li> Loi de godart https://freakonometrics.hypotheses.org/61681
<li> ! données d'apprentissage $\neq$ données de test !
<li> boucle de rétroaction
<li> variable discriminante et variable latente inobservée https://twitter.com/gchampeau/status/1363816748668092416
</ul>

</section>

<script src="../../library/d3.v3.min.js"></script>
<script src="../../library/stack.v1.min.js"></script>
<link rel="stylesheet" href="../../library/styles/hybrid.css">
<script src="../../library/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script>

var mystack = stack()
    .on("activate", activate)
    .on("deactivate", deactivate);

var section = d3.selectAll("section"),
    follow = d3.select("#follow"),
    followAnchor = d3.select("#follow-anchor"),
    lorenz = d3.select("#lorenz"),
    followIndex = section[0].indexOf(follow.node()),
    lorenzIndex = section[0].indexOf(lorenz.node());

function refollow() {
  followAnchor.style("top", (followIndex + (1 - mystack.scrollRatio()) / 2 - d3.event.offset) * 100 + "%");
}

function activate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", refollow);
  if (i === lorenzIndex) startLorenz();
}

function deactivate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", null);
  if (i === lorenzIndex) stopLorenz();
}


</script>
