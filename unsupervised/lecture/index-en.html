<!DOCTYPE html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<style>

@import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz:400,700);
@import url(https://fonts.googleapis.com/css?family=Contrail+One:400,700);

</style>
<link rel="stylesheet" type="text/css" href="../../library/common.css" />
<link rel="stylesheet" type="text/css" href="../../library/screen.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../../library/print.css" media="print" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>

</head>
<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Introduction to ML </span></h1></br> <h2><p class="grey">Master, Paris Est MLV</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>

<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Unsupervised learning</span></h1></br> <h2>Clustering and dimensionality Reduction<br> <p class="grey">Master, Paris Est-MLV</h2>
  <a href="http://www.twiter.com/comeetie">@comeetie</a>
</section>

<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Clustering,</span></h1></br> <h2>Find homogeneous groups</h2>
</section>


<section style="text-align:center;padding-top:5em;">
  <h1><span class="green">Dimensionality reduction</span></h1></br> <h2>Find interesting subspace to project the data</h2>
</section>


<section style="background-color:white;color:black">

<h1 class="green" >Unsupervised learning</h1>


<h3 class="red"> Clustering</h3>

<ul>
<li> K-means
<li> Hierarchical clustering(CAH) 
<li> DB-Scan 
</ul>

<h3 class="red"> Dimensionality reduction</h3>

<ul>
<li> ACP
<li> <span style="color:#888">ICA, SOM, ... </span>
</ul>

</section>

<section style="background-color:white;color:black">

<h1 class="green" >Clustering ?</h1>


<h3>Find <span class="red">"homogeneous"</span> groups</h3>

<ul>
<li> Data : 
</ul>
$$\{\mathbf{x}_1,...,\mathbf{x}_N\}, \mathbf{x_i} \in \mathcal{R}^p$$

</section>
<section style="background-color:white;color:black">

<h1 class="green" >Clustering ?</h1>

<h3>Find <span class="red">"homogeneous"</span> groups</h3>

<img src="./images/plot_cluster.png" height=30%>
<h4 style="text-align:center">Toy examples</h4>
</section>


<section style="text-align:center;padding-top:8em">
<h1>K-means</h1>

</section>

<section style="background-color:white;color:black">
<h1 class="blue"> How to measure the homogeneity ?</h1>
mean distance to a center, variance :
$$\sum_{i=1}^{N}||\mathbf{x}_i-\mu||^2$$
homogeneous groups, small variance around cluster centers
$$\sum_{i=1}^N\sum_{k=1}^{K}z_{ik}||\mathbf{x}_i-\mu_k||^2$$
with : $\forall i\in \{1,...N\},\, z_{ik}\in\{0,1\}^K,\,\sum_{k=1}^{K}z_{ik}=1$
</section>

<section style="background-color:white;color:black">
<h1 class="blue"> K-means</h1>
intra-cluster variance minimization :
$$z_1,...,z_N\,and\,\mathbf{\mu_1},...,\mathbf{\mu_K}=\arg\min\sum_{i=1}^N\sum_{k=1}^{K}z_{ik}||\mathbf{x}_i-\mu_k||^2$$
avec : $\forall i\in \{1,...N\},\, z_{ik}\in\{0,1\}^K,\,\sum_{k=1}^{K}z_{ik}=1$
<p>
<h3 class="red">Problem :</h3>
Computationaly hard, non convex -> multiples "local minima"
</p>
</section>

<section style="background-color:white;color:black">
<h1 class="blue"> K-means</h1>
intra-cluster variance minimization :
$$\arg_{z_1,...,z_N}\min_{\mathbf{\mu_1},...,\mathbf{\mu_K}}\sum_{i=1}^N\sum_{k=1}^{K}z_{ik}||\mathbf{x}_i-\mu_k||^2$$
<h3 class="red">Solution :</h3>
Alternating opitmisation
<ul>
<li> When $z_1,...z_N$ are fixed, it's easy to find the $\mu_1,...,\mu_k$ they are simple mean
<li> When the $\mu_1,...,\mu_K$ are fixed, it's easy to optimize with respect to $z_1,...,z_N$ simply find the closet center for each data-point
<h3 class="red">Iterative algorithm that converge towards a local minima</h3>
<h3 class="red">! initialisation see k-means++</h3>
</section>




<section style="background-color:white;color:black">
<h1 class="blue"> K-means example</h1>
<iframe width="100%" height="70%" scrolling="no" src="./kmeans.html" style="border: none;"></iframe>
</section>

<section style="background-color:white;color:black">
<h1 class="blue"> K-means</h1>
intra-cluster variance minimization :
$$\arg_{z_1,...,z_N}\min_{\mathbf{\mu_1},...,\mathbf{\mu_K}}\sum_{i=1}^N\sum_{k=1}^{K}z_{ik}||\mathbf{x}_i-\mu_k||^2$$
<h3 class="red">Remarks : </h3>
<ul>
<li> We must specify $K$
<li> We implicit assume that cluster looks like ball around "K" centers
<li> We implicitly assume that the groups have similar sizes
</ul>
<h3 class="red">! beyond k-means mixture of gaussian and the EM algorithm</h3>
</section>

<section style="background-color:white;color:black">
<h1 class="blue"> K-means</h1>
<h3> with scikit-lean </h3>
kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)</br>
kmeans.fit(digits.data)</br>
kmeans.cluster_centers_</br>
kmeans.labels_</br>
</section>

<section style='text-align:center;padding-top:5em'>
<h1 > Hierarchical</h1>
<h1>Clustering</h1>
</section>



<section style="background-color:white;color:black">
<h1 class="green"> How to measure homogeneity (2)</h1>
<ul>
<li>Homogeneous groups = groups of similar points
<li>2 points are similar if they are closes 
<li>2 groups of points  $(C_1,C_2)$ are similar if ?
</ul>
The maximum distance between two members of the groups is small:
$$ max_{i \in C_1,j\in C_2}||x_i-x_j||^2$$
<h3 class="green">Maximal link</h3>
</section>


<section style="background-color:white;color:black">
<h1 class="green"> How to measure homogeneity (2)</h1>
<ul>
<li>Homogeneous groups = groups of similar points
<li>2 points are similar if they are closes 
<li>2 groups of points  $(C_1,C_2)$ are similar if ?
</ul>
The minimum distance between two members of the groups is small:
$$ min_{i \in C_1,j\in C_2}||x_i-x_j||^2$$
<h3 class="green">Minimal link</h3>
</section>

<section style="background-color:white;color:black">
<h1 class="green"> How to measure homogeneity (2)</h1>
<ul>
<li>Homogeneous groups = groups of similar points
<li>2 points are similar if they are closes 
<li>2 groups of points  $(C_1,C_2)$ are similar if ?
</ul>
The average distance between two members of the groups is small:
$$ \frac{1}{N_{C_1}N_{C_2}}\sum_{i \in C_1}\sum_{j\in C_2}||x_i-x_j||^2$$
<h3 class="green">Average link</h3>
</section>



<section style="background-color:white;color:black">
<h1 class="green"> How to measure homogeneity (2)</h1>
<ul>
<li>Homogeneous groups = groups of similar points
<li>2 points are similar if they are closes 
<li>2 groups of points  $(C_1,C_2)$ are similar if ?
</ul>
The wheighted distance between their centers is small:
$$ \frac{N_{C_1}N_{C_2}}{N_{C_1}+N_{C_2}}||\mu_{C_1}-\mu_{C_2}||^2$$
<h3 class="green">Ward</h3>
</section>


<section style="background-color:white;color:black">
<h1 class="green">HC algorithm</h1>
<ul>
<li> Put each data point into a cluster
<li> merge the two closest clusters (with respect to the link function)  
<li> until you end up with a unique clsuer with all data-points inside
<li> Build the binary tree that store the merge moves and their costs (dendogram) 
<li> (optional) Cut the tree at a certain level to obtain a flat clustering
</ul>
</section>


<section style="background-color:white;color:black">
<h1 class="green"> HC example</h1>
<h1 style="text-align:center;padding-top:3em"><a href="./agglo.html">"./agglo.html"</a> </h1>
</section>

<section style="background-color:white;color:black">
<h1 class="green">HC</h1>
Greedy optimisation of merge
<h3 class="red">Remarks : </h3>
<ul>
<li> No need to fix $K$ a priori but use the dendogram to fix it
<li> Choice of the distance between cluster is important and has a clear impact on the results
<li> No strong hypothesis on the groups shapes
</ul>
</section>


<section style="background-color:white;color:black">
<h1 class="blue"> HC</h1>
<h3> with scikit-lean </h3>
cah = AgglomerativeClustering(linkage="ward",n_clusters=10)</br>
cah.fit_predict(digits.data)</br>
cah.labels_</br>
</section>

<section style="padding-top:5em;text-align:center">
<h1>DB-Scan</h1> 
<img src="./images/joy-division.jpg" height="60%"></img>
</section>

<section style="background-color:white;color:black">
<h1 class="red"> How to measure homogeneity (3)</h1>
<ul>
<li>homogeneous = in the same high density area
</ul>
<h3 class="green">Density ~ number of data-point per surface area</h3>
</section>


<section style="background-color:white;color:black">
<h1 class="red"> How to measure homogeneity (3)</h1>
<ul>
<li>homogeneous = in the same high density area
</ul>
<h3 class="green">Density ~ number of data-point per surface area</h3>
local measure of the density $\xi(\mathbf{x},\epsilon) = \sum_{i=1}^{N}1_{\{||\mathbf{x}-\mathbf{x}_i\||^2<\epsilon\}}$
<ul>
<li>High density :
 $$\xi(\mathbf{x},\epsilon) > minpts $$
<li>$i,j$ <span class="red">belong to the same high density area</span> =<span class="red"> they belong to the same cluster</span> if :
<p>their is a path between $i$ and $j$ along wich the density is always high</p>
</ul>
</section>



<section style="background-color:white;color:black">
<h1 class="red"> How to measure homogeneity (3)</h1>
<ul>
<li>homogeneous = in the same high density area
<li>$i,j$ <span class="red">belong to the same high density area</span> =<span class="red"> they belong to the same cluster</span> if :
<p>their is a path between $i$ and $j$ along wich the density is always high</p>
</ul>
<img src="./images/dbscan.png" height="60%"></img>
</section>


<section style="background-color:white;color:black">
<h1 class="red">DBSCAN </h1>
parameters :$\epsilon, minpts$</br>
nbcluster = 0
<ul>
<li> For each $\mathbf{x_i}$ :
<ul> 
<li> If $\mathbf{x_i}$ was already visited continue;
<li> Else :</br>

<ul>
<li> If $\xi(\mathbf{x}_i,\epsilon)>minpts$ </br>
nbcluster++</br>
expand($\mathbf{x}_i$,nbcluster)</br>
<li> Else put $\mathbf{x}_i$ in the noise cluster : $c_i=0$
</section>

<section style="background-color:white;color:black">
<h1 class="red">DBSCAN</h1>
expand($\mathbf{x}_i$,nbcluster)</br>
<ul>
<li> $c_i=nbcluster$
<li> $Neighbours = \{\mathbf{x}_j :\, ||\mathbf{x}_j-\mathbf{x}_i||^2<\epsilon\}$
<li> For each $\{\mathbf{x}_j\, \in\, Neigbours\}$
<ul> 
<li> If $\mathbf{x}_j$ was not visited
<ul>
<li> Flag $\mathbf{x_j}$ as visited
<li> If $\xi(\mathbf{x}_j,\epsilon)>minpts$ </br>
$$Neirghbors = Neighbours \cup \{\mathbf{x}_l :\, ||\mathbf{x}_l-\mathbf{x}_j||^2<\epsilon\}$$ 
<li> If $\mathbf{x}_j$ do not belong to any cluster $c_j=nbcluster$
</section>




<section style="background-color:white;color:black">
<h1 class="red">DBSCAN</h1>
<h3 style="padding-top:3em">Remarks</h3>
<ul>
<li> Based on local density (no hypothesis on cluster shapes)
<li> 2 parameters 
<li> Extract a noise cluster 
<li> Quite speed
<ul> 
</section>



<section style="background-color:white;color:black">
<h1 class="blue">DB-scan</h1>
<h3> with scikit-lean </h3>
db = DBSCAN(eps=25,min_samples=40)</br>
db.fit_predict(digits.data)</br>
db.labels_
</section>

<section style="background-color:white;color:black">
<h1 class="green">Clustering, general remarks :</h1>
<h3><span class="red">!! Warning when features have &ne; units !!<span></h3>
<ul>
<li>! preprocessing : centering - reduction </span>
$$\tilde{\mathbf{x}}=(\mathbf{x}-\bar{\mathbf{x}}).\mathbf{S}^{-1}$$
<li>substraction of the mean (centering : mean =0),
<li>divide by the standard deviation (reduction : variance=1) 
<li> normalisation $[0,1]$ substract the min divide by the range.
</ul>
<h3><span class="red">!! size effect & collinearity !!<span></h3>
<ul>
<li>divide by the size "feature" , ex : population
<ul> 
</section>


<section style="padding-top:5em;text-align:center">
<h1 class="green">Dimensionality reduction</h1>
<h1>PCA</h1>
</section>


<section style="padding-top:5em;background-color:white;color:black">
<h1 class="green">Dimensionality reduction</h1>
<ul>
<li> Visualisation, exploration, spped up calculus,... 
<li> Keep the maximum of information
<li> Linear Projection (ACP,ICA) / non-linéaire (SOM, isomap, ...)
</ul>
</section>





<section style="padding-top:5em;text-align:center">
<h1>P</h1>
<h1>C</h1>
<h1>A</h1>
</section>


<section style="text-align:center;background-color:white;color:black">
<h1>PCA</h1>
<img src="./images/Karl_Pearson_line_of_best_fit.jpg" height="60%">
Pearson, K., <i>On Lines and Planes of Closest Fit to Systems of Points in Space</i>, Philosophical Magazine, vol. 2, no 6,‎ 1901, p. 559–572. </i><a href="http://archive.wikiwix.com/cache/?url=http%3A%2F%2Fstat.smmu.edu.cn%2Fhistory%2Fpearson1901.pdf">(pdf)</a>
</section>

<section style="background-color:white;color:black">
<h1 class="green">PCA</h1>
<h3 class="red"> projected variance maimization </h3>
  <img src="./images/ex_acp.png" height="60%"/>

</section>


<section style="background-color:white;color:black">
<h1 class="green">PCA</h1>
<h3 class="red"> reconstruction error minimisation</h3>

  <img src="./images/ex_acp.png" height="60%"/>
</section>



<section style="background-color:white;color:black">
<h1 class="green">PCA</h1>
<h3 class="red"> reconstruction error minimisation</h3>

  <img src="./images/ex_acp.png" height="60%"/>

<span class="red">+ orthogonales composants.</span>
</section>


<section style="background-color:white;color:black">
<h1 class="green">PCA</h1>
<h3 class="red"> projected variance maximisation</h3></br></br>
$$\hat{\mathbf{w}}=\arg\max_{\mathbf{w}}\frac{1}{N}\frac{\mathbf{w}^{t}\mathbf{X}^{t}\mathbf{X}\mathbf{w}}{\mathbf{w}^{t}\mathbf{w}},\,avec\,||\mathbf{w}||=1$$
</section>


<section style="background-color:white;color:black">
<h1 class="green">PCA</h1>
<h3 class="red"> Projected variance maximisation</h3>
Eigenvector <span class="green">$(\mathbf{A}.\mathbf{v}=\lambda.\mathbf{v})$</span> dominant (with the biggest eigenvalue) 
of <span class="red">empirical variance-covariance matrice</span> :
<span class="red">$$\Sigma=\frac{1}{N}\mathbf{X}^{t}\mathbf{X}$$</span>
</section>


<section style="background-color:white;color:black">
<h1 class="green">PCA</h1>
<h3 class="red"> Projected variance maximisation</h3>
Eigenvector <span class="green">$(\mathbf{A}.\mathbf{v}=\lambda.\mathbf{v})$</span> dominant (with the biggest eigenvalue) 
of <span class="red">empirical variance-covariance matrice</span> :
<span class="red">$$\Sigma\mathbf{w}=\lambda_k\mathbf{w}$$</span>
</section>

<section style="background-color:white;color:black">
<h1 class="green">PCA</h1>
<h3 class="red"> Projected variance maximisation</h3>
<img src="./images/GaussianScatterPCA.png"  height="70%"></img>
</section>


<section style="background-color:white;color:black">
<h1 class="green">ACP</h1>
<h3 class="red"> Projected variance maximisation on K components</h3>
$K$ leading eigen vectors $(\mathbf{A}.\mathbf{v}=\lambda.\mathbf{v})$ (with the biggest eigen values) 
of <span class="red">empirical variance-covariance matrice</span> :
$$\Sigma\mathbf{w_k}=\lambda_k\mathbf{w}_k$$
The eigen value associated with component $\lambda_k$ gives the amount of variance exaplained by the $k^{th}$ principal component.
</section>



<section style="background-color:white;color:black">
<h1 class="green">ACP</h1>
<h3 class="red"> Projected variance maximisation on K components</h3>
Projection matrice product:
$$\tilde{\mathbf{X}}_k=\mathbf{X}\mathbf{w_k}$$
<h3 class="red"> Remarks :</h3>
<ul>
<li> Linear method
<li> Simple, easy to compute
<li> Can be used as a preprocessing (regression, classification,...)
<li> ! Reduction and centering if feature in differents units
</ul>
</section>


<section style="background-color:white;color:black">
<h1 class="blue">ACP</h1>
<h3> avec scikit-lean </h3>
pca = decomposition.PCA(n_components=6)</br>
pca.fit(X)</br>
Xp = pca.transform(X)
</section>


<section style="padding-top:5em;text-align:center">
<h1>Self</h1>
<h1>Organizing</h1>
<h1>Maps</h1>
</section>

<section style="padding-top:5em;text-align:center">
<h1>S</h1>
<h1>O</h1>
<h1>M</h1>
</section>

<section style="background-color:white;color:black">
<h1 class="green">SOM</h1>
<h3 class="red"> Discret non linear projection</h3>
<ul>
<li> K-means + grille 
<li> $\Rightarrow$ organize the cluster inside a 2d grid
<li> feature space // grid space
</ul>

<img src="./images/grille.png" height="35%"></img>

</section>

<section style="background-color:white;color:black">
<h1 class="green">SOM</h1>
<ul>
<li> sample randomly a data point $x_c$ 
<li> find the closest (neurone, prototype, cluster) 
<li> $k^* = \arg\min_k||x_c-\mu_k||$
<li> update the (neurones, prototypes, clusters) to be closer from $x_c$
<li> prototypes nearh $k^*$ will be more impacted than distant ones
<li> $\mu_. = \mu_. + \alpha*h(k^*,.,\sigma)*(x_c-\mu_.)$
</ul>

<img src="./images/gauss_kernel.png" height="40%"></img>

</section>



<section style="background-color:white;color:black">
<h1 class="green"> SOM example</h1>
<h1 style="text-align:center;padding-top:3em"><a href="./som.html">som.html</a> </h1>
</section>

<section style="background-color:white;color:black">
<h1 class="green">SOM</h1>
<h3 class="red"> Visualisation, exploration</h3>


<img src="./images/Synapse_Self-Organizing_Map.png" height="55%"></img>

</section>


<section style="background-color:white;color:black">
<h1 class="green"><a href="http://scikit-learn.org">Scikit Learn</a></h1>
<img src="./images/ml_map.png" height="85%"></img>

</section>

<script src="../../library/d3.v3.min.js"></script>
<script src="../../library/stack.v1.min.js"></script>
<link rel="stylesheet" href="../../library/styles/hybrid.css">
<script src="../../library/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script>

var mystack = stack()
    .on("activate", activate)
    .on("deactivate", deactivate);

var section = d3.selectAll("section"),
    follow = d3.select("#follow"),
    followAnchor = d3.select("#follow-anchor"),
    lorenz = d3.select("#lorenz"),
    followIndex = section[0].indexOf(follow.node()),
    lorenzIndex = section[0].indexOf(lorenz.node());

function refollow() {
  followAnchor.style("top", (followIndex + (1 - mystack.scrollRatio()) / 2 - d3.event.offset) * 100 + "%");
}

function activate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", refollow);
  if (i === lorenzIndex) startLorenz();
}

function deactivate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", null);
  if (i === lorenzIndex) stopLorenz();
}


</script>
