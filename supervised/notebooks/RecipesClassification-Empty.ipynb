{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe classification\n",
    "\n",
    "Imagine a colaborative recipe website which wants to propose an automatic tag function which categorize a proposed recipe among a set of recipe type like french, mexican, italian, korean, .... This functionality will be build thanks to a classifier wich will take as input the ingredients of the recipe and will output the recipe type.\n",
    "\n",
    "In this exercice we will build such a classifier using a set of labelled recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "First we need to read and encode the dataset for further processing. The \"recipe_train.json\" contains an array of recipe with a cuisine filed with the recipe cuisine type (a string) and an ingredients fileds with an array of ingredients (strings). We first load the data and build an np.array of cuisine type to predict (y) and a list of recipes ingredients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./recipes_train.json\") as f:\n",
    "    data_train=json.load(f)\n",
    "    y = np.array([recipe[\"cuisine\"] for recipe in data_train])\n",
    "    xtext = [recipe[\"ingredients\"] for recipe in data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must encode the ingredients and define the numeric features that will describe our recipes. To do so we will take a classic approach in text processing called bag of words so in our case bag of ingredients. Each recipe will be then associated with a big vectors of zeros and one. Each element of this vector will correspond to an ingredients and we will put a one if the recipe use this ingredients and a zero if it's not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so we start by computing the list of all the ingredients and a dict which associated to each ingredients an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = np.unique(np.concatenate(xtext))\n",
    "dict_ingredients = dict((ingredients[i],i) for i in range(0, len(ingredients)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may then encode the recipe using a small function that will take an array of ingredients and return a vector of size (number of possible ingredients) with ones at the right places. Eventually we stack all these vectors in our data matrix X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(recipe):\n",
    "    x = np.zeros((1,len(ingredients)))\n",
    "    indices = [dict_ingredients[ing] for ing in recipe]\n",
    "    x[0,indices]=1\n",
    "    return x\n",
    "X = np.vstack([encode(recipes) for recipes in xtext])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is quite voluminous and since our computational ressource are scarses we will remove the columns that corresponds to rarely used ingredients and select only the recipes of 10 types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ingredients=ingredients[np.sum(X,axis=0)>100]\n",
    "selected_type= np.array(['chinese', 'french', 'greek', 'indian', 'italian', 'jamaican',\n",
    "       'korean', 'mexican', 'moroccan', 'thai'])\n",
    "Xs=X[:,np.sum(X,axis=0)>100]\n",
    "Xs=Xs[np.isin(y,selected_type),:]\n",
    "ys=y[np.isin(y,selected_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is now ready to be processed. The final list of ingredients that we will used to recognize the cuisine types is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the types of cuisines that we must recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training and test set\n",
    "\n",
    "As usually, we will split the data into a training and a test dataset. the \"train_test_split\" function from scikit learn is dedicated to this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = ## TO ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "We will use randomForest to solve the classification problem as a first try. We will fit a randomforest with \n",
    " trees and estimate the accuracy of such classifier using cross_validation and the test set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "\n",
    "\n",
    "clf = ## DEF CLASSIFIER\n",
    "scores = ## UTILISER LA FONCTION DE CROSS VALIDATION POUR CLASSIFIER LES RECETTE\n",
    "\n",
    "score.## PRINT MEAN SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also simply fit this model with the training data and produce prediction for the test set with the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN THE MODEL\n",
    "## USE PREDICT FONCTION ON TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute the accuracy with numpy or the built-in function of scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPUTE THE ACCURACY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THE ACCURACY FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others metrics can be computed with the classification report tools from scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRINT THE CLASSIFICATION REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we may compute the detailled result with the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AFFICHER LA MATRICE DE CONFUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "### 1) What is the accuracy of this classifier on training data? What do you conclude?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Try to improve the performance of this classifier in terms of accuracy ?\n",
    "You can vary the number of trees between [20,50,100,200]. Use the GridSearchCV function seen in the first notebbok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Test another solution (Logistic regression)\n",
    "You will be able to read the documentation of sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Try to implement a rejection solution, to obtain an accuracy of at least 90%? \n",
    "To do this you will use the probabilities provided by the classifier and will only make a decision when these probabilities are above a certain threshold (to be determined). The predict_proba method should be able to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)  Try to solve the problem with more features with logistic regression.  Try to use the regularization parameter and compare the results of Multinomial and One-vs-Rest for handling the multiclass problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ingredients=ingredients[np.sum(X,axis=0)>50]\n",
    "selected_type= np.array(['chinese', 'french', 'greek', 'indian', 'italian', 'jamaican',\n",
    "       'korean', 'mexican', 'moroccan', 'thai'])\n",
    "Xs=X[:,np.sum(X,axis=0)>20]\n",
    "Xs=Xs[np.isin(y,selected_type),:]\n",
    "ys=y[np.isin(y,selected_type)]\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.3, random_state=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## DEFINE A HYPER PARAMETER GRID\n",
    "hyper_params = [{\"penalty\":[\"none\"],\"multi_class\":[\"ovr\",\"multinomial\"]},\n",
    "                {\"penalty\":[\"l2\"],\"multi_class\":[\"ovr\",\"multinomial\"],\"C\":np.linspace(0.1,2,num=10)}]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THE GRID SEARCH WITH THE HYPERPARAMETER GRID TO GET THE BEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIT YOUR GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRINT SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res = pd.DataFrame(grid_res.cv_results_[\"params\"])\n",
    "res[\"mean_accuracy\"]=grid_res.cv_results_[\"mean_test_score\"]\n",
    "res[\"std_accuracy\"]=grid_res.cv_results_[\"std_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res.loc[res[\"multi_class\"]==\"ovr\",[\"C\"]],res.loc[res[\"multi_class\"]==\"ovr\",[\"mean_accuracy\"]],label=\"ovr\")\n",
    "plt.plot(res.loc[res[\"multi_class\"]==\"multinomial\",[\"C\"]],res.loc[res[\"multi_class\"]==\"multinomial\",[\"mean_accuracy\"]],label=\"multinomial\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
